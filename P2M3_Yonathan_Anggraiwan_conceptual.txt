# Milestone 3
___

## Introduction
Nama  : Yonathan Anggraiwan

Batch : 028

Program ini dibuat untuk melakukan end-to-end data pipeline dari PostgreSQL ke ElasticSearch. Adapun dataset yang dipakai adalah dataset data karyawan yang didapatkan dari situs [Kaggle](https://www.kaggle.com/datasets/mexwell/employee-performance-and-productivity-data). 

Dalam project kali ini, juga dilakukan validasi data menggunakan teknik Great Expecations, dan visualisasi data yang ditampilkan dalam dashboard interaktif dari Kibana.
___

1. NoSQL adalah jenis database non-relasional yang tidak menyimpan data 
dalam bentuk tabel seperti RDBMS. NoSQL biasanya digunakan untuk data yang 
strukturnya fleksibel, seperti dokumen JSON, data key value, dan data grafik. 

2. A. Menggunakan NoSQL disaat:
- Struktur data tidak tetap atau fleksibel (misalnya format JSON, log, dokumentasi dinamis).
- Sistem perlu untuk menangani data dalam jumlah yang besar.

B. Menggunakan RDBMS disaat:
- Struktur tabel dan relasi antar data sudah jelas dan stabil.
- Dibutuhkan integritas dan validasi data yang kuat.

3. 2 Contoh Tools NoSQL Selain ElasticSearch:
a. MongoDB
- Tipe: Document Store
- Keunggulan:
- Struktur JSON fleksibel (tidak perlu skema data yang tetap)
- Cocok untuk rapid development dan iterasi produk
- Mirip SQL

b. Redis
- Tipe: Key-Value Store
- Keunggulan:
- Super cepat untuk caching dan real time analytics
- Cocok untuk leaderboard, session store, queue

4. Airflow adalah platform untuk membuat, menjadwalkan, dan memonitor alur 
kerja data pipeline. Biasanya digunakan untuk melakukan eksekusi yang terjadwal 
(batch), dan didefinisikan dalam bentuk DAG (Directed Acyclic Graph).

Contohnya: setiap malam jam 1 pagi, tarik data dari API → simpan ke PostgreSQL → jalankan model ML → kirim hasilnya via email.

Airflow bagus karena:
- Fleksibel: bisa menggabungkan Python, Bash, SQL
- Observabilitas tinggi: setiap task bisa dimonitor, retry, dll
- Mudah diintegrasikan dengan tools seperti Docker, PostgreSQL, dll.


5.Great Expectations adalah library Python untuk validasi kualitas data secara otomatis. 
Kita bisa bikin "expectation" (semacam aturan kualitas) untuk memastikan data sesuai harapan: 
a. nilai tidak null, 
b. tipe data benar, 
c. kolom unique,
d. dll.

Fitur utamanya:
- Dapat dipakai inline saat eksplorasi data (pakai validator)
- Dapat dijalankan secara otomatis dalam pipeline (Airflow, dbt, dll)
- Output dapat divisualisasikan dalam bentuk report HTML interaktif

6.Batch Processing adalah pendekatan pemrosesan data dalam jumlah besar secara berkala 
(biasanya dijalankan dalam batch harian/mingguan). Tidak seperti stream yang real time, 
batch bersifat terjadwal dan volume datanya besar.

Contoh Penggunaan:
- Menghitung total transaksi harian
- Preprocessing data sebelum masuk ke dashboard
- Menjalankan model prediksi churn tiap malam

Tools Batch yang Umum:
- Apache Airflow (orchestrator)
- Apache Spark (engine untuk proses data besar)
- dbt (transformasi SQL modular)
- Cron (penjadwalan sederhana)